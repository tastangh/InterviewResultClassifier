Epoch	Training Loss	Validation Loss
1	10.4422	2.5076
2	10.3035	10.3616
3	10.4751	10.3616
4	10.4456	10.3616
5	10.4150	10.3616
6	10.3938	10.3616
7	10.3850	10.3616
8	10.3772	10.3616
9	10.3753	10.3616
10	10.3781	10.3616
11	10.3830	10.3616
12	10.3882	10.3616
13	10.3907	10.3616
14	10.3732	10.3616
15	10.3379	10.3616
16	10.3730	10.3616
17	10.3535	10.3616
18	10.3718	10.3616
19	10.3660	10.3616
20	10.3463	10.3616
21	10.3796	10.3616
22	10.3865	10.3616
23	10.3296	10.3616
24	9.4140	10.3616
25	10.3921	10.3616
26	10.3734	10.3616
27	10.3439	10.3616
28	10.3146	10.3616
29	9.9423	10.3616
30	10.5971	10.3616
31	10.7475	10.3616
32	10.7387	10.3616
33	10.7291	10.3616
34	10.7026	10.3616
35	10.6738	10.3616
36	10.6434	10.3616
37	10.6131	10.3616
38	10.5798	10.3616
39	10.5426	10.3616
40	10.5062	10.3616
41	10.4714	10.3616
42	10.4311	10.3616
43	10.3958	10.3616
44	10.3712	10.3616
45	10.3581	10.3616
46	10.3529	10.3616
47	10.3437	10.3616
48	10.3133	10.3616
49	10.2978	10.3616
50	9.3309	10.3616
51	10.6120	10.3616
52	10.5755	10.3616
53	10.5392	10.3616
54	10.5033	10.3616
55	10.4683	10.3616
56	10.4351	10.3616
57	10.3956	10.3616
58	10.3635	10.3616
59	10.3423	10.3616
60	10.3324	10.3616
61	10.3306	10.3616
62	10.3312	10.3616
63	10.3209	10.3616
64	10.2890	10.3616
65	10.3313	10.3616
66	10.3457	10.3616
67	10.3803	10.3616
68	8.6894	10.3616
69	9.9580	10.3616
70	10.5827	10.3616
71	10.5465	10.3616
72	10.5104	10.3616
73	10.4747	10.3616
74	10.4398	10.3616
75	10.4067	10.3616
76	10.3688	10.3616
77	10.3363	10.3616
78	10.3144	10.3616
79	10.3040	10.3616
80	10.3025	10.3616
81	10.3054	10.3616
82	10.3073	10.3616
83	10.2928	10.3616
84	10.2619	10.3616
85	9.3091	10.2211
86	10.3553	10.3616
87	10.4824	10.3616
88	10.4533	10.3616
89	10.4235	10.3616
90	10.3941	10.3616
91	10.3666	10.3616
92	10.3371	10.3616
93	10.3034	10.3616
94	10.2797	10.3616
95	10.2671	10.3616
96	10.2641	10.3616
97	10.2671	10.3616
98	10.2733	10.3616
99	10.2803	10.3616
100	10.2852	10.3616
